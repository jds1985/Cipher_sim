export const runtime = "nodejs";
// pages/api/chat.js
// Cipher OS â€” stable core (Gemini-compatible, hardened)

import { runCipherCore } from "../../cipher_core/core.js";
import { loadMemory, saveMemory } from "../../cipher_core/memory.js";

import { buildOSContext } from "../../cipher_os/runtime/osContext.js";
import { runOrchestrator } from "../../cipher_os/runtime/orchestrator.js";

import {
  loadMemoryNodes,
  loadSummary,
  saveSummary,
} from "../../cipher_os/memory/memoryGraph.js";

import { writebackFromTurn } from "../../cipher_os/memory/memoryWriteback.js";

export default async function handler(req, res) {
  if (req.method !== "POST") {
    return res.status(405).json({ error: "Method not allowed" });
  }

  try {
    const message = req.body?.message?.trim() || "Hello";

    const userId = "jim";
    const userName = "Jim";

    // â”€â”€ TRACE LOGGER (ğŸ” THIS IS THE MISSING PIECE) â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const trace = {
      log: (event, payload) => {
        console.log(`[TRACE] ${event}`, payload ?? "");
      },
    };

    trace.log("request.received", {
      messageLength: message.length,
    });

    // â”€â”€ Load long-term memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const memoryData = await loadMemory(userId);
    const longTermHistory = memoryData?.history || [];

    trace.log("memory.loaded", {
      longTermTurns: longTermHistory.length,
    });

    // â”€â”€ Load memory graph â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const nodes = await loadMemoryNodes(userId, 60);
    const summaryDoc = await loadSummary(userId);

    trace.log("memoryGraph.loaded", {
      nodes: nodes?.length || 0,
      hasSummary: Boolean(summaryDoc?.text),
    });

    // â”€â”€ Build OS context â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const osContext = buildOSContext({
      requestId: Date.now().toString(),
      userId,
      userName,
      userMessage: message,
      uiHistory: [],
      longTermHistory,
    });

    osContext.memory.nodes = nodes;
    osContext.memory.longTermSummary = summaryDoc?.text || "";

    trace.log("osContext.built", {
      requestId: osContext.requestId,
    });

    // â”€â”€ Executive layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const executivePacket = await runCipherCore(
      {
        history: osContext.memory.mergedHistory,
        nodes,
        summary: osContext.memory.longTermSummary,
      },
      { userMessage: message, returnPacket: true }
    );

    trace.log("executive.complete", {
      hasSystemPrompt: Boolean(executivePacket?.systemPrompt),
    });

    // â”€â”€ Orchestrator (Gemini â†’ OpenAI â†’ Anthropic) â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const out = await runOrchestrator({
      osContext,
      executivePacket,
      trace, // ğŸ”¥ THIS enables model-level telemetry
    });

    trace.log("orchestrator.complete", {
      modelUsed: out?.modelUsed || null,
    });

    const reply =
      typeof out === "string"
        ? out
        : out?.reply || out?.text || null;

    if (!reply) {
      console.error("âŒ Orchestrator returned no reply", out);
      return res.status(500).json({
        error: "Model produced no reply",
      });
    }

    // ğŸ” Normalize model name for UI badge
    const model =
      out?.model ||
      out?.modelUsed ||
      out?.engine ||
      null;

    // â”€â”€ Save memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    await saveMemory(userId, {
      type: "interaction",
      role: "user",
      content: message,
    });

    await saveMemory(userId, {
      type: "interaction",
      role: "assistant",
      content: reply,
    });

    trace.log("memory.saved", {
      userTurnSaved: true,
      assistantTurnSaved: true,
    });

    // â”€â”€ Memory graph writeback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    await writebackFromTurn({
      userId,
      userText: message,
      assistantText: reply,
    });

    trace.log("memoryGraph.writeback", {
      completed: true,
    });

    // â”€â”€ Keep summary doc updated (no AI summarizer) â”€â”€â”€â”€â”€â”€â”€
    const turns = (summaryDoc?.turns || 0) + 1;
    await saveSummary(userId, summaryDoc?.text || "", turns);

    trace.log("summary.updated", {
      turns,
    });

    // âœ… UI-BADGE SAFE RESPONSE
    return res.status(200).json({
      reply,
      model, // â† UI badge reads this
    });
  } catch (err) {
    console.error("âŒ /api/chat fatal error:", err);
    return res.status(500).json({
      error: err.message || "Chat failed",
    });
  }
}
